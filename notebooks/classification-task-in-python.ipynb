{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2cc8665-7c9f-4966-97b1-661c938f3717",
   "metadata": {},
   "source": [
    "Classification Task <br>\n",
    "Input: text data <br>\n",
    "Output: Copom's decision (keep, raise, lower) interest rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a318ba6-affa-4565-bea2-6a7ead27c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "import eli5\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, tree, neural_network, neighbors, ensemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e40439-8700-4cc0-b8e2-4b5f1c0a0926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n",
    "plt.rcParams[\"figure.figsize\"] = (60,30)\n",
    "plt.rcParams['figure.dpi'] = 90\n",
    "plt.rcParams.update({'font.size': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c47e0-d925-4488-9256-e4799d86e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "AtasFolder = \"../atas\"\n",
    "listAtas = os.listdir(AtasFolder)\n",
    "corpus = []\n",
    "meeting = []\n",
    "for ata in listAtas:\n",
    "    meeting.append(int(re.search(\"[0-9]+\",ata).group()))\n",
    "    with open(AtasFolder + \"/\" + ata,'rt', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        if lines: \n",
    "            lines = ' '.join(lines)\n",
    "            corpus.append(lines)\n",
    "\n",
    "print(len(corpus),\"atas\")\n",
    "dfCorpus = pd.DataFrame(corpus, index = meeting, columns=[\"corpus\"])\n",
    "del corpus, meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510067e-b827-4822-8a3a-611b07ec7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = pd.read_csv(\"../decisions.csv\")\n",
    "decisions.index = np.int64(decisions.meeting.values)\n",
    "decisions = decisions[\"decision\"]\n",
    "decisions = decisions.to_frame(name=\"decisions\")\n",
    "decisions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d570de-fcfb-464e-9746-3fca232adeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCorpus = dfCorpus.join(decisions)\n",
    "del decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f47047-2e37-4483-a994-f1c2e90213a0",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8eff43-987a-4ff7-a848-bed06ffc5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mystopwords = ['acordar','agora','ainda','aladi','alegrar','além','antar','ante','anthero','antonio','apenas','apesar','apresentação','aquém','araújo','cada','capitar','carioca','carteiro','contra','corpus','corrêa','costa','daquela','demais','diante','edson','entanto','estar','estevar','então','feltrim','final','finar','geral','içar','ie','intuito'] + \\\n",
    "['le','luiz','luzir','mediante','meirelles','mercar','moraes','necessariamente','neto','of','oficiar','oliveira','onde','ora','parir','paulo','pelar','pesar','pilar','pois','primo','quadrar','reinar','res','resinar','reunião','ser','sob','sobre','somente','sr','tal','tais','tanto','thomson','tipo','todo','tony','usecheque','vasconcelos'] + \\\n",
    "['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'] + \\\n",
    "['um','dois','três','quatro','cinco','seis','sete','oito','nove','dez','onze','doze','treze','catorze','quinze','dezesseis','dezessete','dezoito','dezenove','vinte'] + \\\n",
    "['aquela','aquelas','aquele','aqueles','àquela','àquelas','daquele','daqueles','daquela','daquelas','naquele','naqueles','naquela','naquelas','neste','nesta','nestes','nestas','nisto','nesse','nessa','nesses','nessas','nisso'] + \\\n",
    "['janeiro','fevereiro','março','abril','maio','junho','julho','agosto','setembro','outubro','novembro','dezembro','mês','meses','ano','anos'] + [str(i) for i in range(10)] + nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "print(\"Number of stopwords: \",len(Mystopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee565a9b-d92d-49cf-b2aa-af721ad7be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dfCorpus.corpus.to_list()\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i]= corpus[i].lower()\n",
    "    corpus[i] = re.sub('\\n', ' ', corpus[i]) # remove newline\n",
    "    corpus[i] = re.sub('[0-9]+', ' ', corpus[i]) #remove numbers\n",
    "    corpus[i] = re.sub(r'[^\\w\\s]',' ',corpus[i]) #remove punctuation\n",
    "    corpus[i] = re.sub('º','',corpus[i])\n",
    "    corpus[i] = re.sub('ª','',corpus[i])\n",
    "    corpus[i] = re.sub('@','',corpus[i])\n",
    "    corpus[i] = re.sub('#','',corpus[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225658c6-8179-4fc7-879b-bdc213278bba",
   "metadata": {},
   "source": [
    "## lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3c570-f0ab-4b4a-b856-cde025b83e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#large portuguese model\n",
    "nlp = spacy.load('pt_core_news_lg', disable=['parser', 'ner'])\n",
    "\n",
    "for i in range(len(corpus)): \n",
    "    doc = nlp(corpus[i]) \n",
    "    corpus[i]=\" \".join([token.lemma_ for token in doc]) \n",
    "\n",
    "# fix wrong lemmas\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i] = re.sub(\"atar\",\"ata\", corpus[i])\n",
    "    corpus[i] = re.sub(\"agregar\",\"agregado\", corpus[i])\n",
    "    corpus[i] = re.sub(\"atuais\",\"atual\", corpus[i])\n",
    "    corpus[i] = re.sub(\"barreirar\",\"barreira\", corpus[i])\n",
    "    corpus[i] = re.sub(\"bolhar\",\"bolha\", corpus[i])\n",
    "    corpus[i] = re.sub(\"comerciar\",\"comércio\", corpus[i])\n",
    "    corpus[i] = re.sub(\"comer\",\"como\", corpus[i])\n",
    "    corpus[i] = re.sub(\"conjuntar\",\"conjunto\", corpus[i])\n",
    "    corpus[i] = re.sub(\"cifrar\",\"cifra\", corpus[i])\n",
    "    corpus[i] = re.sub(\"curvar\",\"curva\", corpus[i])\n",
    "    corpus[i] = re.sub(\"demandar\",\"demanda\", corpus[i])\n",
    "    corpus[i] = re.sub(\"desalentar\",\"desalento\", corpus[i])\n",
    "    corpus[i] = re.sub(\"marginar\",\"marginal\", corpus[i])\n",
    "    corpus[i] = re.sub(\"mediano\",\"mediana\", corpus[i])\n",
    "    corpus[i] = re.sub(\"mear\",\"meio\", corpus[i])\n",
    "    corpus[i] = re.sub(\"mercar\",\"mercado\", corpus[i])\n",
    "    corpus[i] = re.sub(\"meter\",\"meta\", corpus[i])\n",
    "    corpus[i] = re.sub(\"ofertar\",\"oferta\", corpus[i])\n",
    "    corpus[i] = re.sub(\"oitavar\",\"oitavo\", corpus[i])\n",
    "    corpus[i] = re.sub(\"orar\",\"ora\", corpus[i])\n",
    "    corpus[i] = re.sub(\"parir\",\"para\", corpus[i])\n",
    "    corpus[i] = re.sub(\"picar\",\"pico\", corpus[i])\n",
    "    corpus[i] = re.sub(\"queda\",\"quedo\", corpus[i])\n",
    "    corpus[i] = re.sub(\"redar\",\"rede\", corpus[i])\n",
    "    corpus[i] = re.sub(\"resultar\",\"resultado\", corpus[i])\n",
    "    corpus[i] = re.sub(\"riscar\",\"risco\", corpus[i])\n",
    "    corpus[i] = re.sub(\"segundar\",\"segundo\", corpus[i])\n",
    "    corpus[i] = re.sub(\"trazido\",\"trazer\", corpus[i])\n",
    "    corpus[i] = re.sub(\"votar\",\"voto\", corpus[i])\n",
    "    \n",
    "dfCorpus[\"clean_corpus\"] = corpus\n",
    "del corpus, doc, i, nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944b4a5-652b-4eb5-9c1d-eaf9cd4ee389",
   "metadata": {},
   "source": [
    "# Train, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fac43e-d138-4ac3-bfba-3b0a455542f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "\n",
    "# Divisão dos textos em um conjunto de treinamento e outro de validação\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(dfCorpus.clean_corpus.to_list(), dfCorpus.decisions.to_list(), \n",
    "                                                                      test_size=0.30, \n",
    "                                                                      random_state = 100, \n",
    "                                                                      stratify=dfCorpus.decisions.to_list() )\n",
    "print(\"Train:\",len(X_train),len(y_train))\n",
    "print(\"Test:\",len(X_test),len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e1bf0-5c05-430f-bab3-56d077d35ead",
   "metadata": {},
   "source": [
    "# Encoder labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75380a7-026e-40ae-84f5-ca95b47dde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy labels\n",
    "y_train_labels = y_train.copy()\n",
    "y_test_labels = y_test.copy()\n",
    "\n",
    "#Tratamento dos dados de saída\n",
    "# Codificação das variveis alvo da classificação\n",
    "encoder = preprocessing.LabelEncoder() #criação do codificador\n",
    "encoder.fit(dfCorpus.decisions)\n",
    "y_train = encoder.transform(y_train) #codificação dos dados de treinamento\n",
    "y_test = encoder.transform(y_test) #codificação dos dados de validação\n",
    "labels = encoder.classes_ #criação de uma lista contendo os tipos de norma (classes da classificação)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79932c-a677-4e73-a128-799cb2731c49",
   "metadata": {},
   "source": [
    "# tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aafd22-9c17-427c-8356-39b7729c3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 2000\n",
    "#DTM-TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                             stop_words=Mystopwords,\n",
    "                             max_df = 0.8,\n",
    "                             min_df = 0.1,\n",
    "                             #ngram_range=(1, 2), \n",
    "                             max_features = max_tokens)\n",
    "tfidf_vect.fit(X_train) \n",
    "\n",
    "print(\"tfidf:\",len(tfidf_vect.get_feature_names_out()),\" tokens\")\n",
    "\n",
    "X_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "list_words = list(tfidf_vect.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a84f6-6a68-4d97-b12e-cc01e9a94067",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06edf7f1-d88f-4e53-ad58-7dcfd0576663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, train_x, train_y, test_x, test_y, parameters = None):\n",
    "    \"\"\"\n",
    "    #classifier: sklearn classifier model\n",
    "    #train_x: train data input (X)\n",
    "    #train_y: train data output (Y)\n",
    "    #test_x: test data input (X)\n",
    "    #test_y: test data output(Y)\n",
    "    #parameters: classifier's parameters for GridSearch\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    if (__name__ == \"__main__\") & (parameters != None) :\n",
    "        # multiprocessing requires the fork to happen in a __main__ protected\n",
    "        # block\n",
    "\n",
    "        # find the best parameters for both the feature extraction and the\n",
    "        # classifier\n",
    "        grid_search = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=0,cv=5)\n",
    "        grid_search.fit(train_x, train_y)\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        predictions= grid_search.best_estimator_.predict(test_x)\n",
    "        classifier = grid_search.best_estimator_\n",
    "        \n",
    "    else:\n",
    "        # train the classifier\n",
    "        classifier.fit(train_x, train_y)\n",
    "        # make predictions\n",
    "        predictions = classifier.predict(test_x)\n",
    "    \n",
    "\n",
    "    #calcula a matriz de confusão\n",
    "    confusionMatrix(predictions, test_y)\n",
    "    print(\"\\n\") #pula uma linha\n",
    "    #cria um relatório com base nas previsões realizdas\n",
    "    classificationReport(predictions, test_y)\n",
    "    \n",
    "    #calcula o kapppa\n",
    "    kappa = metrics.cohen_kappa_score(test_y, predictions)\n",
    "    print(\"Kappa score: {:.3f}\\n\".format(kappa))\n",
    "    acc = metrics.accuracy_score(test_y, predictions)\n",
    "    print(\"Accuracy score: {:.3f}\\n\".format(acc))\n",
    "    f1 = metrics.f1_score(test_y, predictions, average='weighted')\n",
    "    print(\"f1 weighted score: {:.3f}\\n\".format(f1))\n",
    "    acc_bal = metrics.balanced_accuracy_score(test_y, predictions)\n",
    "    print(\"Balanced Accuracy score: {:.3f}\\n\".format(acc_bal))\n",
    "    \n",
    "    # retorna a acurácia do modelo        \n",
    "    return  classifier\n",
    "\n",
    "def confusionMatrix(predictions, real):\n",
    "    X = np.array( metrics.confusion_matrix(y_true=real,y_pred=predictions))\n",
    "    X = pd.DataFrame(X,index = labels, columns = labels)\n",
    "    print(X)\n",
    "    return\n",
    "\n",
    "\n",
    "def classificationReport(predictions, real):\n",
    "    print(metrics.classification_report(y_true=real,y_pred=predictions, target_names=labels))    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f657bd-a4a1-4ae2-8745-2c6de63e3760",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69255dbe-1121-4eda-83ff-578662737298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECISION TREE\n",
    "nome = \"DECISION TREE\"\n",
    "DecisionTreeModel = tree.DecisionTreeClassifier()\n",
    "parameters_ = {'criterion': ('gini', 'entropy'),\n",
    "               'splitter':('best','random'),\n",
    "               'max_depth':(10, 20, 40, 50, None),\n",
    "               'class_weight' : ('balanced',None)\n",
    "               }\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "DecisionTreeModel = train_model(DecisionTreeModel, X_train_tfidf, y_train, X_test_tfidf, y_test, parameters = parameters_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf9b3d-dce4-4f69-b6a0-de97c96157ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(DecisionTreeModel, max_depth=3, feature_names=list_words, class_names=labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff813fbc-6a7c-41b7-b2df-d8114eadb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(DecisionTreeModel, top=10, target_names=labels, feature_names=list_words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30ff6603-db88-437c-9414-3905500303b2",
   "metadata": {},
   "source": [
    "eli5.show_prediction(DecisionTreeModel, X_test[0][:1000], vec=tfidf_vect, target_names=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c43e8-ec50-4c2b-a174-2b8faf749e64",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab30d5b5-42fb-43b8-b23e-3c229da0a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGISTIC REGRESSION\n",
    "nome = \"Logistic Regression\"\n",
    "LogisticRegressionModel = linear_model.LogisticRegression()\n",
    "parameters_ = {'C':(0.5, 1.0),\n",
    "               'class_weight' : ('balanced',None)}\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "LogisticRegressionModel = train_model(LogisticRegressionModel, X_train_tfidf, y_train, X_test_tfidf, y_test, parameters = parameters_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e43eb-a475-4593-a95c-666e40d647e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefLogReg = pd.DataFrame({'words': list_words,\n",
    "                      'keep': np.reshape( LogisticRegressionModel.coef_.tolist()[0] , (LogisticRegressionModel.coef_.shape[1],)),\n",
    "                      'lower': np.reshape( LogisticRegressionModel.coef_.tolist()[1] , (LogisticRegressionModel.coef_.shape[1],)),\n",
    "                      'raise': np.reshape( LogisticRegressionModel.coef_.tolist()[2] , (LogisticRegressionModel.coef_.shape[1],))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f41ac6-8f2b-4b63-ae0d-2403a3cdff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    print(\"\\n- Words correlated with: \"+labels[i]+\"\\n\\t\", coefLogReg.sort_values(by=labels[i], ascending=False).head(10).words.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91485e-2d70-4c82-8c31-a769d74aba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(LogisticRegressionModel, top=10, target_names=labels, feature_names=list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84c775-8040-46ad-a5ff-e987f85ed29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(LogisticRegressionModel, X_test[0][:1000], vec=tfidf_vect, target_names=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20b344-4839-4139-a9b2-384d28d1f2e5",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c851a-b74d-42e2-a79e-5e647e8f60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "nome = \"SVM\"\n",
    "SVMModel = svm.SVC()\n",
    "parameters_ = {'C': (0.5, 1.0),\n",
    "               'kernel':(['linear']),\n",
    "               'class_weight' : ('balanced',None)}\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "SVMModel = train_model(SVMModel, X_train_tfidf, y_train, X_test_tfidf, y_test, parameters = parameters_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85eab8-db60-4775-ab16-f64d84df1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_importances(coef, names, nrWords, title):\n",
    "    imp,names = zip(*sorted(zip(coef,names), reverse=True))\n",
    "    plt.barh(range(nrWords), imp[:nrWords], align='center')\n",
    "    plt.yticks(range(nrWords), names[:nrWords])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"weight\")\n",
    "    plt.ylabel(\"words\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f61963-00fd-4525-8601-17fcd5a30571",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_importances(SVMModel.coef_[0].todense().tolist()[0], list_words, 10, labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b0dc0-f683-4c32-9eb3-f82267dc2506",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_importances(SVMModel.coef_[1].todense().tolist()[0], list_words, 10, labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7d332-67e8-4d09-8fe9-c1d8a24385ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_importances(SVMModel.coef_[2].todense().tolist()[0], list_words, 10, labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3148f11-3eb9-4b6e-8f25-000d4f46d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefSVM = pd.DataFrame({'words': list_words,\n",
    "                      'keep': np.reshape( SVMModel.coef_.todense().tolist()[0] , (SVMModel.coef_.shape[1],)),\n",
    "                      'lower': np.reshape( SVMModel.coef_.todense().tolist()[1] , (SVMModel.coef_.shape[1],)),\n",
    "                      'raise': np.reshape( SVMModel.coef_.todense().tolist()[2] , (SVMModel.coef_.shape[1],))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcd9c0-0d16-4b98-913b-1a170697e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    print(\"\\n- Words correlated with: \"+labels[i]+\"\\n\\t\", coefSVM.sort_values(by=labels[i], ascending=False).head(10).words.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8d061-ed84-46f3-9ca5-410d6f92cce4",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554e20e-44df-4084-807c-8b4563883946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "nome = \"Random Forest\"\n",
    "RandomForestModel = ensemble.RandomForestClassifier(random_state=100)\n",
    "parameters_ = {'n_estimators' : (50, 75, 100),\n",
    "               'criterion': ('gini', 'entropy'),\n",
    "               'max_depth':(20, 40, 50, None),\n",
    "               'class_weight' : ('balanced','balanced_subsample',None)\n",
    "               }\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "RandomForestModel = train_model(RandomForestModel, X_train_tfidf, y_train, X_test_tfidf, y_test, parameters = parameters_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b32943-829f-4626-9a26-5e5de118731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame(data=RandomForestModel.feature_importances_, index=list_words, columns=['importance'])\n",
    "#imp.sort_values('importance', ascending=False)\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in RandomForestModel.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "indices = np.argsort(RandomForestModel.feature_importances_)[::-1]\n",
    "\n",
    "nr_words=20\n",
    "indices = indices[:nr_words]\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "plt.figure(figsize=(15,10));\n",
    "plt.title(\"Random Forest - word importance\")\n",
    "plt.bar(range(nr_words), RandomForestModel.feature_importances_[indices],\n",
    "        color=\"g\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(nr_words), pd.Index(list_words)[indices], rotation=75)\n",
    "plt.xlim([-1, nr_words])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10e523-6062-4170-afd9-f3ec587cddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(RandomForestModel, top=10, target_names=labels, feature_names=list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f1ce3-495c-457c-ad95-c576b33a684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(RandomForestModel, X_test[0][:1000], vec=tfidf_vect, target_names=labels, top=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
